import express from 'express';
import pgClient from './supabaseService.js';
import { getAiRoles, getModels } from './supabaseService.js';

const router = express.Router();

// GET /api/knowledge-base - Get all knowledge base documents
router.get('/knowledge-base', async (req, res) => {
  try {
    if (!pgClient) {
      return res.status(500).json({ error: 'Database not connected' });
    }

    const result = await pgClient.query(
      `SELECT id, name, type, status, upload_date, vector_count, size, content_url
       FROM knowledge_base
       ORDER BY upload_date DESC`
    );

    const documents = result.rows.map((row: any) => ({
      id: row.id,
      name: row.name,
      type: row.type,
      status: row.status,
      uploadDate: row.upload_date,
      vectorCount: row.vector_count,
      size: row.size
    }));

    res.json(documents);
  } catch (error) {
    console.error('Error fetching knowledge base:', error);
    res.status(500).json({ error: 'Failed to fetch knowledge base' });
  }
});

// POST /api/knowledge-base/upload - Upload a new document
router.post('/knowledge-base/upload', async (req, res) => {
  try {
    const { name, type, size } = req.body;

    if (!name || !type) {
      return res.status(400).json({ error: 'Missing required fields: name, type' });
    }

    if (!pgClient) {
      return res.status(500).json({ error: 'Database not connected' });
    }

    // Insert document with PENDING status
    const result = await pgClient.query(
      `INSERT INTO knowledge_base (name, type, size, content_url, status)
       VALUES ($1, $2, $3, $4, 'PENDING')
       RETURNING id, name, type, status, upload_date, vector_count, size, content_url`,
      [name, type, size, null]
    );

    const document = result.rows[0];

    // Start async processing (chunking, embedding, storing)
    processDocumentAsync(document.id, name).catch(err => {
      console.error('Error processing document:', err);
      updateDocumentStatus(document.id, 'FAILED');
    });

    res.json({
      id: document.id,
      name: document.name,
      type: document.type,
      status: document.status,
      uploadDate: document.upload_date,
      vectorCount: document.vector_count,
      size: document.size
    });
  } catch (error) {
    console.error('Error uploading document:', error);
    res.status(500).json({ error: 'Failed to upload document' });
  }
});

// DELETE /api/knowledge-base/:id - Delete a document
router.delete('/knowledge-base/:id', async (req, res) => {
  try {
    const { id } = req.params;

    if (!pgClient) {
      return res.status(500).json({ error: 'Database not connected' });
    }

    // Delete document (this will cascade delete chunks)
    await pgClient.query(
      'DELETE FROM knowledge_base WHERE id = $1',
      [id]
    );

    res.json({ success: true, message: 'Document deleted successfully' });
  } catch (error) {
    console.error('Error deleting document:', error);
    res.status(500).json({ error: 'Failed to delete document' });
  }
});

// Async function to process document (chunk, embed, store)
async function processDocumentAsync(documentId: string, name: string) {
  try {
    console.log(`Starting processing for document: ${name} (${documentId})`);

    // Update status to PROCESSING
    await updateDocumentStatus(documentId, 'PROCESSING');
    console.log(`Status updated to PROCESSING for: ${name}`);

    // Get the configured embedding model from database
    const roles = await getAiRoles();
    const ragModelId = roles.rag;
    const models = await getModels();
    const embeddingModel = models.find(m => m.id === ragModelId);

    if (!embeddingModel) {
      console.warn('No embedding model configured, using default HuggingFace model');
    }

    // Simulate document processing (in real implementation, you would:
    // 1. Download the file from contentUrl
    // 2. Extract text from PDF/DOCX
    // 3. Chunk the text into smaller pieces
    // 4. Generate embeddings for each chunk
    // 5. Store chunks with embeddings in knowledge_chunks table)

    // Simulate text extraction and chunking (shortened delay for demo)
    await new Promise(resolve => setTimeout(resolve, 2000));

    // Generate sample chunks based on document name
    const mockChunks = [
      `Văn bản pháp luật: ${name} - Điều 1 quy định về nguyên tắc chung`,
      `Văn bản pháp luật: ${name} - Điều 2 quy định về đối tượng áp dụng`,
      `Văn bản pháp luật: ${name} - Điều 3 quy định về trình tự thực hiện`,
      `Văn bản pháp luật: ${name} - Điều 4 quy định về hồ sơ cần chuẩn bị`,
      `Văn bản pháp luật: ${name} - Điều 5 quy định về thời hạn giải quyết`,
    ];

    console.log(`Generated ${mockChunks.length} chunks for: ${name}`);

    // Update status to VECTORIZING
    await updateDocumentStatus(documentId, 'VECTORIZING');
    console.log(`Status updated to VECTORIZING for: ${name}`);

    const vectorCount = mockChunks.length;

    // Generate and store embeddings for each chunk
    for (let i = 0; i < mockChunks.length; i++) {
      try {
        const embedding = await generateEmbedding(mockChunks[i], embeddingModel);
        if (embedding && pgClient) {
          await pgClient.query(
            `INSERT INTO knowledge_chunks (knowledge_base_id, content, embedding, chunk_index, metadata)
             VALUES ($1, $2, $3, $4, $5)`,
            [documentId, mockChunks[i], `[${embedding.join(',')}]`, i, { source: name, model: embeddingModel?.model_string || 'default' }]
          );
          console.log(`Stored chunk ${i + 1}/${mockChunks.length} for: ${name}`);
        }
      } catch (chunkError) {
        console.error(`Error processing chunk ${i}:`, chunkError);
        // Continue processing other chunks even if one fails
      }
    }

    // Update status to COMPLETED
    await updateDocumentStatus(documentId, 'COMPLETED', vectorCount);
    console.log(`Document ${name} processed successfully with ${vectorCount} chunks`);

  } catch (error) {
    console.error('Error in processDocumentAsync:', error);
    await updateDocumentStatus(documentId, 'FAILED');
  }
}

// Helper function to update document status
async function updateDocumentStatus(documentId: string, status: string, vectorCount: number | null = null) {
  if (!pgClient) return;

  const query = vectorCount !== null
    ? 'UPDATE knowledge_base SET status = $1, vector_count = $2 WHERE id = $3'
    : 'UPDATE knowledge_base SET status = $1 WHERE id = $3';

  const params = vectorCount !== null ? [status, vectorCount, documentId] : [status, documentId];

  await pgClient.query(query, params);
}

// Helper function to generate embedding
// IMPORTANT: Always returns 1536 dimensions to match database schema (vector(1536))
async function generateEmbedding(text: string, embeddingModel?: any): Promise<number[] | null> {
  try {
    let apiKey = '';
    let apiUrl = '';
    let embeddingSize = 384;

    // Determine which API to use based on model configuration
    if (embeddingModel && embeddingModel.api_key) {
      apiKey = embeddingModel.api_key;
    } else {
      // Fallback to environment variable
      apiKey = process.env.HUGGINGFACE_API_KEY || '';
    }

    if (!apiKey) {
      console.warn('No API key available, using mock embedding');
      return Array(embeddingSize).fill(0).map(() => Math.random());
    }

    // Determine API endpoint based on provider/model
    if (embeddingModel && embeddingModel.provider === 'openai') {
      apiUrl = `https://api.openai.com/v1/embeddings`;
      embeddingSize = 1536;
    } else if (embeddingModel && embeddingModel.provider === 'huggingface') {
      apiUrl = 'https://api-inference.huggingface.co/models/BAAI/bge-small-en-v1.5';
      embeddingSize = 384;
    } else {
      // Default to HuggingFace
      apiUrl = 'https://api-inference.huggingface.co/models/BAAI/bge-small-en-v1.5';
      embeddingSize = 384;
    }

    let response;
    let data;

    if (embeddingModel && embeddingModel.provider === 'openai') {
      // Use OpenAI API
      response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        },
        body: JSON.stringify({
          model: embeddingModel.model_string || 'text-embedding-3-small',
          input: text
        })
      });
      data = await response.json();

      if (response.ok && data.data && Array.isArray(data.data)) {
        return data.data[0].embedding;
      } else {
        console.error('OpenAI API error:', data);
        return Array(embeddingSize).fill(0).map(() => Math.random());
      }
    } else {
      // Use HuggingFace API
      response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        },
        body: JSON.stringify({
          inputs: text
        })
      });
      data = await response.json();

      if (response.ok && Array.isArray(data)) {
        return data[0];
      } else {
        console.error('HuggingFace API error:', data);
        return Array(embeddingSize).fill(0).map(() => Math.random());
      }
    }
  } catch (error) {
    console.error('Error generating embedding:', error);
    return Array(384).fill(0).map(() => Math.random());
  }
}

export default router;
